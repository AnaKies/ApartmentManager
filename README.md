# Apartment Manager API

This project is a Flask-based backend API for an apartment management system. It provides a public-facing chat interface powered by a Large Language Model (LLM) and a set of internal endpoints for direct database management.

## Getting Started

### Prerequisites

Before you begin, ensure you have Python installed on your system. The specific dependencies for this project are not listed in `requirements.txt`, but based on the code, you will need to install the following packages:

*   `flask`
*   `flask-cors`
*   `python-dotenv`
*   `google-generativeai`
*   `requests`
*   `werkzeug`

You can install them using pip:

```bash
pip install flask flask-cors python-dotenv google-generativeai requests werkzeug
```

You will also need to set up a `.env` file in the `ApartmentManager` directory with your `GEMINI_MODEL` environment variable.

```
GEMINI_MODEL=your-gemini-model-name
```

### Running the Application

1.  **Initialize the Database:**
    The application uses an ORM to manage the database. To create the necessary tables, run the `init_orm_db.py` script:

    ```bash
    python ApartmentManager/backend/SQL_API/init_orm_db.py
    ```

2.  **Start the Server:**
    To start the Flask server, run the `flask-api.py` file:

    ```bash
    python ApartmentManager/backend/RESTFUL_API/main.py
    ```

    The server will start on `http://127.0.0.1:5003`.

## API Endpoints

The API is divided into two main blueprints: a public API for chat and an internal API for data management.

### Public API (`/api`)

#### `POST /api/chat`

This endpoint provides a conversational interface for interacting with the system. It takes a user's question and returns an answer generated by an LLM.

**Request Body:**

```json
{
  "user_input": "What is the status of the apartment on the second floor?"
}
```

**Success Response (200 OK):**

The response is the direct output from the LLM.

### Internal API (`/internal`)

These endpoints are intended for internal use and provide direct access to the database.

#### `GET /tenancies`

Retrieves a JSON list of all tenancy records.

#### `GET /rent_data`

Retrieves a JSON list of all rent data records.

#### `GET /persons`

Retrieves a JSON list of all person records.

#### `POST /persons`

Adds a new person to the database. The request body should be a JSON object containing the person's data.

**Request Body Example:**

```json
{
  "first_name": "John",
  "last_name": "Doe",
  "email": "john.doe@example.com"
}
```

#### `GET /apartments`

Retrieves a JSON list of all apartment records.

## Error Handling

The API uses a standardized format for error responses to ensure consistency. Instead of using a simple success/error envelope, it relies on custom error handlers to format exceptions into a detailed JSON object.

All error responses include the following fields:

*   `code`: The error code (either an HTTP status code or a custom application-specific code).
*   `message`: A human-readable description of the error.
*   `llm_model`: The name of the LLM model being used.
*   `answer_source`: Indicates that the error originated from the `backend`.
*   `trace_id`: A unique ID for tracing the error in the logs.

**Example Error Response:**

```json
{
  "code": 400,
  "message": "400 Bad Request: The browser (or proxy) sent a request that this server could not understand.",
  "llm_model": "gemini-2.5-flash",
  "answer_source": "backend",
  "trace_id": "some-unique-trace-id"
}
```

This approach ensures that clients receive structured and informative error messages for different types of issues, including application-level errors, HTTP errors, and issues with the underlying LLM API.
